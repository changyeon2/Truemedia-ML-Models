# Semantic Deepfake Detection

Our [semantic deepfake detection approach](https://github.com/truemediaorg/deepfake-app/blob/main/apps/detect/app/api/starters/gpt-util/transcriptAnalysis.ts) takes audio and video data as input, creates a transcript of the media using the speech recognition model [Whisper](https://openai.com/index/whisper/), and then processes the transcript using an LLM. The LLM outputs a prediction of whether the audio is real or fake based on historical context, meaning, and language use. The LLM prompt has been carefully trained using the [DSPy prompt-training framework](https://dspy.ai/) with a MIPROv2 optimizer. This prompt was optimized for LLM [gpt-4o-2024-08-06](https://platform.openai.com/docs/models#gpt-4o). The prompt was trained on user-uploaded data that was labeled as real and fake by the professional labeling team at TrueMedia. Our experiments showed that the use of a trained prompt is essential for accurate LLM predictions.

This approach is unique because instead of identifying discrepancies in the physical and/or auditory characteristics through, e.g., wavelengths and pixels as is done in traditional deepfake detection, this approach determines the presence of manipulation through an investigation of the textâ€™s underlying language. Predictions are based on language construction, narrative coherence, and factual alignment using an advanced LLM which draws from a variety of historical sources to evaluate the efficacy and veracity of a media content.

The implementation of the transcript pipeline for audio deepfake detection can be found inside the TrueMedia.org application, here: https://github.com/truemediaorg/deepfake-app/blob/main/apps/detect/app/api/starters/gpt-util/transcriptAnalysis.ts
